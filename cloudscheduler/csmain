#!/usr/bin/python3

import os
import sys
import copy
import json
import math
import time
import logging
import multiprocessing
from collections import defaultdict

import cloudmanager
#import cloudscheduler.cloudmanager


from cloudscheduler.lib.schema import view_idle_vms
from cloudscheduler.lib.schema import view_groups_of_idle_jobs
from cloudscheduler.lib.schema import view_available_resources
from cloudscheduler.lib.schema import view_condor_jobs_group_defaults_applied
from cloudscheduler.lib.schema import view_metadata_collation_json

from cloudscheduler.lib.db_config import Config
from cloudscheduler.lib.log_tools import get_frame_info

from cloudscheduler.lib.ec2_translations import get_ami_dictionary

from cloudscheduler.lib.ProcessMonitor import ProcessMonitor

from sqlalchemy import exists
from sqlalchemy.sql import select
from sqlalchemy.exc import TimeoutError


def main():
    """
    main function.
    """
    multiprocessing.current_process().name = "csmain"

    config = Config('/etc/cloudscheduler/cloudscheduler.yaml', 'csmain', pool_size=25)
    config.db_engine.dispose()
    if not config:
        print("Problem loading config file.")
    log = logging.getLogger(__name__)
    logging.basicConfig(filename=config.log_file,
                        level=config.log_level,
                        format="%(asctime)s - %(processName)-12s - %(levelname)s - %(message)s")
    service_catalog_yaml_fqdns = {}
    try:
        config.db_open()
        service_catalog = config.db_map.classes.csv2_service_catalog
        services = config.db_session.query(service_catalog)
        for service in services:
            service_catalog_yaml_fqdns[service.yaml_attribute_name] = service.fqdn
    except TimeoutError as ex:
        log.exception(ex)
        sys.exit(1)
    except Exception as ex:
        log.exception(ex)


    while(True):
        CSGroups = config.db_map.classes.csv2_groups
        try:
            config.db_open()
            csv2groups = config.db_session.query(CSGroups)
        except TimeoutError as ex:
            log.exception(ex)
            sys.exit(1)
        except Exception as ex:
            log.exception(ex)
            config.db_close()
            time.sleep(2)
            continue

        # Get the metadata for groups as json selects with mime_types pre-sorted
        # metadata format: { group : { cloud: [(yaml select, mime_type), (yaml select, mime_type) ] } }
        try:
            ec2_image_dict = get_ami_dictionary()
        except Exception as ex:
            log.exception(ex)
            continue
        metadata = {}
        try:
            metadata = json.loads(config.db_connection.execute(select([view_metadata_collation_json])).fetchone().group_metadata)
        except AttributeError as ae:
            log.exception("Probably loading metadata, is there no metadata?")
        config.db_close()

        for csgroup in csv2groups:
            log.debug("Dealing with current group: %s", csgroup.group_name)
            if metadata and csgroup.group_name in metadata.keys():
                pass
            else:
                log.debug("No metadata/yamls found for group: %s this may cause problem with VMs registering to condor.",
                          csgroup.group_name)

            # Setup the resources for group and sort out their group and cloud specific yamls
            try:
                config.db_open()
                group_resources = config.db_session.query(view_available_resources).filter(view_available_resources.c.group_name == csgroup.group_name).group_by(view_available_resources.c.cloud_name)
                Group_metadata = config.db_map.classes.csv2_group_metadata
                group_yamls = config.db_session.query(Group_metadata).filter(Group_metadata.group_name == csgroup.group_name,
                                                               Group_metadata.enabled == 1)
            except TimeoutError as ex:
                log.exception(ex)
                sys.exit(1)
            except Exception as ex:
                log.exception(ex)
                config.db_close()
                continue
            group_yaml_list = []
            config.db_close()
            for yam in group_yamls:
                group_yaml_list.append([yam.metadata_name, yam.metadata, yam.mime_type, yam.priority])

            cm_group = cloudmanager.CloudManager(name=csgroup.group_name, group_resources=group_resources,
                                                                group_yamls=group_yaml_list,
                                                                metadata=metadata[csgroup.group_name] if csgroup.group_name in metadata.keys() else None)
            cm_group.setup()
            # At this point I should have a valid set of all the group and cloud specific yaml to use
            # it will still need to be combined with the job yaml - done now in the basecloud prepare_userdata function

            # Clean up the idle machines and unregistered VMs.
            # Should return or build up some meta data about what's booting/running
            # To use that info when scheduling idle jobs
            # ie just booted machines should register soon so don't boot more VMs for a small number of jobs.
            #verify_idle_jobs(csgroup.group_name) # TODO figure out this old function

            current_flavors = count_cloud_flavors(csgroup.group_name, config)
            #log.debug("Group: %s Current flavors: %s", csgroup.group_name, current_flavors)
            try:

                config.db_open()
                # Booting up new VMs to fill in any free space on available clouds related to idle queued jobs
                # Get the idle jobs for the current group
                idle_jobs_for_group = config.db_session.query(view_groups_of_idle_jobs).filter(
                    view_groups_of_idle_jobs.c.group_name == csgroup.group_name).order_by(
                    view_groups_of_idle_jobs.c.job_priority)

                log.debug('---------------got idle jobs--------------')
                cloud_booted_on = set()  #  Setting this up here now so I can check it and skip over things I boot on due to available slots info will be wrong
                for idle_job in idle_jobs_for_group:
                    if idle_job.idle == 0:
                        continue
                    log.debug("Info for current job: Group: %s, Target: %s, User: %s, Flavors: %s",idle_job.group_name, idle_job.target_clouds,
                              idle_job.user, idle_job.flavors)
                    # flavors is generated line based on which flavor is the best fit and which clouds have available resources
                    # the format is "Group:Cloud:Flavor, Group:Cloud:Flavor" will need to split and use to filter in resources_matching

                    if idle_job.flavors:
                        flavor_list = [x.strip() for x in idle_job.flavors.split(',')]
                    else:
                        continue

                    if idle_job.target_clouds:
                        target_cloud_list = [x.strip() for x in idle_job.target_clouds.split(',')]
                        # Issue #36
                        if not set(target_cloud_list).intersection(set(cm_group.clouds.keys())):
                            log.warning("No Target Cloud found in currently available or enabled clouds. for jobs: %s", idle_job)
                            continue
                            # TODO Flag job to hold it? these jobs are grouped though, and IDs aren't listed in view, makes it harder to pinpoint them.
                            # Either need to query them somehow, or see if it's possible for that view to list/include all the IDs in that set. Ask Colin

                    else:
                        target_cloud_list = []

                    # Get All the possible matching clouds
                    try:

                        if target_cloud_list:
                            clouds_matching = config.db_session.query(view_available_resources).filter(
                                view_available_resources.c.flavor.in_(flavor_list),
                                view_available_resources.c.cloud_name.in_(target_cloud_list),
                                view_available_resources.c.group_name == csgroup.group_name)
                        else:
                            clouds_matching = config.db_session.query(view_available_resources).filter(
                                view_available_resources.c.group_name == csgroup.group_name,
                                view_available_resources.c.flavor.in_(flavor_list))
                    except TimeoutError as ex:
                        log.exception(ex)
                        sys.exit(1)
                    except Exception as ex:
                        log.exception(ex)
                        time.sleep(2)
                        continue

                    num_vm_jobs_per_flavor_total = {}
                    total_cpus = idle_job.request_cpus_total
                    total_ram = idle_job.request_ram_total
                    total_disk = idle_job.request_disk_total
                    log.debug("Resource requests for job: Total Cores: %s, Total Mem: %s, Total Disk: %s", total_cpus, total_ram, total_disk)
                    for flavor in flavor_list:
                        cloud_nm, flv_name = flavor.split(':')
                        log.debug('subtract existing resources for cloud: %s. %s VMs with flavor: %s, C: %s, M: %s, D: %s', cloud_nm, current_flavors[cloud_nm][flavor]['count'], flv_name,
                                  current_flavors[cloud_nm][flavor]['cores'], current_flavors[cloud_nm][flavor]['ram'],
                                  current_flavors[cloud_nm][flavor]['disk'])
                        total_cpus -= current_flavors[cloud_nm][flavor]['cores']
                        total_ram -=  current_flavors[cloud_nm][flavor]['ram']
                        total_disk -= current_flavors[cloud_nm][flavor]['disk']
                    log.debug("Resource requests After checking current: Total Cores: %s, Total Mem: %s, Total Disk: %s", total_cpus,
                              total_ram, total_disk)
                    if total_ram > 0 or total_cpus > 0 or total_disk > 0:
                        # Looks like we need more VMs, but how many?
                        for cloud_match in clouds_matching:
                            vm_cpu_total = total_cpus / cloud_match.flavor_cores
                            vm_ram_total = total_ram / cloud_match.flavor_ram
                            vm_disk_total = total_disk / cloud_match.flavor_disk if cloud_match.flavor_disk else None
                            vm_jobs_possible_total = int(max(vm_cpu_total, vm_ram_total, vm_disk_total)) if vm_disk_total else int(max(vm_cpu_total, vm_ram_total))
                            if vm_jobs_possible_total < 1:
                                vm_jobs_possible_total = 1
                            num_vm_jobs_per_flavor_total[cloud_match.flavor] = vm_jobs_possible_total
                        log.debug("num vms per flavor(total): %s", num_vm_jobs_per_flavor_total)
                        # Now have the amount of jobs that can run on each option
                        # Next get all the VMs for this group with their flavor info - actually could be done outside of loop and re-use it
                        # Need a structure of the groups VMs accessible by the cloud_name and with a count of the flavor
                        # ie vms[cloud-name][flavor] = 6

                        log.debug("Running: %s, Idle: %s", idle_job.running, idle_job.idle)
                        log.debug("flavor list: %s", flavor_list)
                        try:

                            if target_cloud_list:
                                clouds_matching_new = config.db_session.query(view_available_resources).filter(
                                    view_available_resources.c.flavor.in_(flavor_list),
                                    view_available_resources.c.cloud_name.in_(target_cloud_list),
                                    view_available_resources.c.group_name == csgroup.group_name) # .order_by(view_available_resources.c.priority)
                            else:
                                clouds_matching_new = config.db_session.query(view_available_resources).filter(
                                    view_available_resources.c.flavor.in_(flavor_list),
                                    view_available_resources.c.group_name == csgroup.group_name)  # .order_by(view_available_resources.c.priority)
                        except TimeoutError as ex:
                            log.exception(ex)
                            sys.exit(1)
                        except Exception as ex:
                            log.exception(ex)
                            time.sleep(1)
                            continue

                        for cloud in clouds_matching_new:
                            if cloud.cloud_name in cloud_booted_on:
                                continue
                            log.debug("Taking a look at booting on: %s", cloud.cloud_name)
                            log.debug("Using Flavor: %s", cloud.flavor)
                            log.debug("flavor slots: %s", cloud.flavor_slots)
                            log.debug("Using Totals counts: %s", num_vm_jobs_per_flavor_total)

                            num_vms_to_boot = num_vm_jobs_per_flavor_total[cloud.flavor]
                            log.debug("Initially try boot: %s based on possible per flavor", num_vms_to_boot)
                            # This is the total wanted, but cloud may not be able to handle that request
                            num_vms_to_boot = num_vms_to_boot if num_vms_to_boot <= cloud.flavor_slots else cloud.flavor_slots
                            num_vms_to_boot = math.floor(num_vms_to_boot)
                            log.debug("Try to boot: %s after checking flavor slots.", num_vms_to_boot)
                            if num_vms_to_boot == 0:
                                log.debug("Don't try to boot anything - continue")
                                continue
                            # Throttle amount of VMs to boot in one shot - should this be per cloud?
                            # TODO To support more load balancing on clouds modify this section to split this amount
                            # TODO across the clouds of the current cloud priority value. ALternatively figure out some kind of quota usage and sort the results on that similar to csv1
                            if num_vms_to_boot > config.max_start_vm_cloud:
                                num_vms_to_boot = config.max_start_vm_cloud
                                log.debug("Adjusting for max_boot, will only boot: %s vms.", num_vms_to_boot)

                            log.debug("Would like to boot %s VMs on %s", num_vms_to_boot, cloud.cloud_name)
                            try:
                                usertmp = idle_job.user.split('@')[0]
                            except:
                                usertmp = idle_job.user
                            template_dict = {'cs_user': usertmp,
                                             'cs_group_name': csgroup.group_name,
                                             'cs_condor_host': csgroup.htcondor_fqdn,
                                             'cs_condor_name': csgroup.htcondor_container_hostname,
                                             'cs_condor_submitters': csgroup.htcondor_other_submitters,
                                             'cs_cloud_alias': idle_job.target_alias_clouds}
                            template_dict.update(service_catalog_yaml_fqdns)
                            log.debug(template_dict)

                            try:
                                if cm_group.clouds[cloud.cloud_name].enabled:
                                    use_image = None
                                    if idle_job.image:
                                        use_image = None
                                    elif cloud.default_image:
                                        use_image = cloud.default_image
                                    else:
                                        pass
                                    if cloud.cloud_type == 'amazon':
                                        if idle_job.image:
                                            use_image = ec2_image_dict[cm_group.name][cloud.cloud_name][idle_job.image]
                                        elif cloud.default_image:
                                            use_image = ec2_image_dict[cm_group.name][cloud.cloud_name][cloud.default_image]

                                    cm_group.clouds[cloud.cloud_name]\
                                        .vm_create(num=int(num_vms_to_boot),
                                                   flavor=cloud.flavor,
                                                   job=idle_job,
                                                   template_dict=template_dict,
                                                   image = use_image)
                                    log.debug('done booting on cloud %s', cloud.cloud_name)
                                    cloud_booted_on.add(cloud.cloud_name)
                                    break

                            except Exception as ex:
                                # lets try to disable or dump this cloud for now
                                log.exception("Disable cloud %s due to exception(later).", cloud.cloud_name)
                                #cm_group.clouds[cloud_match.cloud_name].enabled = False
                                # TODO Need logic for re-enable before stopping things and forgetting about them.
                        else:
                            log.debug("Hit the end of clouds_matching_new, anything boot or nothing?")
                    else:  # We have enough, wait for things to register
                        log.debug("Seem to have enough VMs for the number of idle jobs.")
                        continue

                config.db_close()
            except TimeoutError as ex:
                log.exception(ex)
                sys.exit(1)
            except Exception as ex:
                log.exception(ex)
                config.db_close()
                time.sleep(2)
                continue
        time.sleep(15)  # delay between groups


def verify_idle_jobs(group, config):
    """ Check the view for idle jobs and make sure it has all the required fields
    Hold jobs that fail the tests."""
    log = logging.getLogger(__name__)
    log.debug("Verify jobs for group: %s", group)
    # TODO Review this function to see if it even still works and does what it's supposed to.
    # TODO Make this stand alone and spin it out into a separate multiprocessing piece
    try:
        config.db_open()
        idle_jobs_for_group = config.db_session.query(view_condor_jobs_group_defaults_applied).filter(
            view_condor_jobs_group_defaults_applied.c.group_name == group,
            view_condor_jobs_group_defaults_applied.c.job_status == 1)
        bad_jobs = {}
        for job in idle_jobs_for_group:
            if not job.image:
                bad_jobs[job.global_job_id] = "Missing Image"
        condor_jobs = config.db_map.classes.condor_jobs
        jobs = config.db_session.query(condor_jobs).filter(condor_jobs.global_job_id.in_(bad_jobs.keys()))

        for job in jobs:
            try:
                job.hold_job_reason = bad_jobs[job.global_job_id]
                config.db_session.merge(job)
            except KeyError:
                continue
            except Exception as ex:
                log.exception(ex)
                continue
        config.db_close(commit=True)
    except TimeoutError as ex:
        log.exception(ex)
        sys.exit(1)
    except Exception as ex:
        log.exception(ex)
        config.db_close()
        return
    log.debug("Done Verify jobs.")
    return


def count_cloud_flavors(group, config):
    """Query the VMs for group and arrange into a dictionary format
    of dict[cloud_name][flavor] = count of that flavor
    """
    log = logging.getLogger(__name__)
    try:
        config.db_open()
        available = config.db_session.query(view_available_resources).filter(view_available_resources.c.group_name == group)
        vm_cloud_flavors = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))
        for res in available:
            count = res.flavor_VMs - res.flavor_error - res.flavor_retiring
            # TODO flavor_name gone - only have flavor with the cloud:name format
            # TODO Can the nested structure go away and just use flavor?
            vm_cloud_flavors[res.cloud_name][res.flavor]['count'] = count
            vm_cloud_flavors[res.cloud_name][res.flavor]['cores'] = count * res.flavor_cores
            vm_cloud_flavors[res.cloud_name][res.flavor]['ram'] = count * res.flavor_ram
            vm_cloud_flavors[res.cloud_name][res.flavor]['disk'] = count * res.flavor_disk
            vm_cloud_flavors[res.cloud_name][res.flavor]['swap'] = count * res.flavor_swap
        log.debug("Done counting current flavors for group: %s.", group)
        config.db_close()
    except TimeoutError as ex:
        log.exception(ex)
        sys.exit(1)
    except Exception as ex:
        log.exception(ex)
        config.db_close()
        return None
    return vm_cloud_flavors


def check_view_idle_vms():
    """Query view_idle_vms and retire as needed."""
    multiprocessing.current_process().name = "csmain_idle_vms"
    log = logging.getLogger(__name__)
    config = Config('/etc/cloudscheduler/cloudscheduler.yaml', 'csmain', pool_size=15)
    config.db_engine.dispose()
    if not config:
        print("Problem loading config file.")
        return
    while True:
        log.debug("-------------------check_view_idle_vms-------------------")
        try:
            config.db_open()
            results = config.db_session.query(view_idle_vms).filter(view_idle_vms.c.retire == 0,
                                                                    view_idle_vms.c.terminate == 0)
            vmids = []
            for res in results:
                vmids.append(res.vmid)
            if vmids:
                log.debug("Setting retire flag on %s VMs.", len(vmids))
                vms = config.db_map.classes.csv2_vms
                update_result = config.db_session.query(vms).filter(vms.vmid.in_(vmids))
                for row in update_result:
                    row.retire = 1
                    old_updater = row.updater
                    row.updater = get_frame_info() + ":r1"
                    config.db_session.merge(row)
                    log.debug("Set retire flag on %s, previous updater: %s", (row.hostname, old_updater))
            config.db_close(commit=True)
        except TimeoutError as ex:
            log.exception(ex)
            sys.exit(1)
        except Exception as ex:
            log.exception(ex)
            config.db_close()
        time.sleep(10)  # TODO make configurable.

def check_unregistered_machines():
    """Query the condor machines and cloud VMs to sort out which ones have failed to register correctly.
    and take steps to shut those down. There could be problems with shutting down machines cs isn't controlling
    so need to account for the hostnames matching the CS hostname pattern, belonging to correct group etc."""
    log = logging.getLogger(__name__)
    multiprocessing.current_process().name = "csmain_unregistered_vms"
    config = Config('/etc/cloudscheduler/cloudscheduler.yaml', 'csmain', pool_size=15)
    config.db_engine.dispose()
    while True:
        log.debug("--------------------Check Unregistered Machines--------------------")
        try:
            config.db_open()
            machines = config.db_map.classes.condor_machines
            vms = config.db_map.classes.csv2_vms
            clouds = config.db_map.classes.csv2_clouds
            group_clouds = config.db_session.query(clouds)
            unregistered_vms = config.db_session.query(vms).filter(~exists().
                                                                   where(machines.machine.contains(vms.hostname)))
            cloud_names = []
            for cloud in group_clouds:
                cloud_names.append(cloud.cloud_name)

            new_vms = defaultdict(list)
            to_terminate = []
            log.debug("VMs that are unregistered with condor collector:")
            for vm in unregistered_vms:
                try:
                    hostname_split = vm.hostname.split('--')
                    if len(hostname_split) == 1:
                        continue
                    cname = hostname_split[1]
                    if cname in cloud_names: # name prefix matches a valid cloud name
                        if int(time.time()) - vm.last_updated > config.unregistered_machine_time_limit and not vm.terminate: # not registered after some period of time
                            to_terminate.append(vm.vmid)
                            log.debug("Set terminate flag on vm due to exceeding registration time limit: %s", vm.hostname)
                        elif not vm.terminate and vm.retire:
                            to_terminate.append(vm.vmid)
                            log.debug("Set terminate flag on vm has been retired: %s", vm.hostname)
                        else:
                            log.debug("%s unregistered for: %s seconds. Terminate flag: %s", vm.hostname,
                                      str(int(time.time()) - vm.last_updated), vm.terminate)
                            new_vms[vm.cloud_name].append((vm.cloud_name,vm.hostname,vm.flavor_id))
                except Exception as ex:
                    log.exception("Problem going through unregistered VMs: %s", ex)
            if to_terminate:
                log.debug("Setting flag in db on %s VMs. With IDs: %s", len(to_terminate), to_terminate)
                update_result = config.db_session.query(vms).filter(vms.vmid.in_(to_terminate))
                for vm in update_result:
                    log.debug("Update terminate flag on %s, previous updater: %s", (vm.hostname, vm.updater))
                    vm.terminate = 1
                    vm.updater = get_frame_info() + ":t=1"
                    config.db_session.merge(vm)
        except TimeoutError as ex:
            log.exception(ex)
            sys.exit(1)
        except Exception as ex:
            log.exception(ex)
            config.db_close()
            return
        log.debug("Done checking unregistered VMs.")
        try:
            config.db_close(commit=True)
        except TimeoutError as ex:
            log.exception(ex)
            sys.exit(1)
        except Exception as ex:
            log.exception(ex)
        time.sleep(10)  # TODO Make configurable ?

if __name__ == '__main__':

    process_ids = {
        'scheduler': main,
        'idle_vms': check_view_idle_vms,
        #'unregistered_vms': check_unregistered_machines,  #  No longer dealing with killing VMs.
    }

    procMon = ProcessMonitor(config_params=[os.path.basename(sys.argv[0]), "csmain"], pool_size=15,
                             orange_count_row='csv2_scheduler_error_count', process_ids=process_ids)
    config = procMon.get_config()
    log = procMon.get_logging()
    version = config.get_version()

    log.info(
        "**************************** starting CSv2 Scheduler processes - Running %s *********************************" % version)

    # Wait for keyboard input to exit
    try:
        # start processes
        procMon.start_all()
        while True:
            procMon.check_processes()
            time.sleep(config.sleep_interval_main_long)

    except (SystemExit, KeyboardInterrupt):
        log.error("Caught KeyboardInterrupt, shutting down threads and exiting...")

    except Exception as ex:
        log.exception("Process Died: %s", ex)

    procMon.join_all()
