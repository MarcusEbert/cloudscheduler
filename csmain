#!/usr/bin/python3
import os
import copy
import time
import logging
from collections import defaultdict

import cloudscheduler.jobmanager
import cloudscheduler.cloudmanager
import cloudscheduler.config as csconfig

from lib.schema import view_vms
from lib.schema import view_groups_of_idle_jobs
from lib.schema import view_available_resources

from sqlalchemy import create_engine
from sqlalchemy import exists
from sqlalchemy.orm import Session
from sqlalchemy.ext.automap import automap_base


CLOUDRESOURCES = {}

logging.basicConfig(level=logging.DEBUG,
                    format="%(asctime)s:%(levelname)s:%(name)s:%(message)s")


def main():
    """
    main function.
    """
    log = logging.getLogger(__name__)
    while(True):
        # basic scheduling: fifo w/ condor_prioity

        Base0 = automap_base()
        engine0 = create_engine("mysql+pymysql://" + csconfig.config.db_user + ":" + csconfig.config.db_password + "@" +
                     csconfig.config.db_host + ":" + str(csconfig.config.db_port) + "/" + csconfig.config.db_name)
        Base0.prepare(engine0, reflect=True)
        CSGroups = Base0.classes.csv2_groups
        session0 = Session(engine0)
        # Get all the current csv2_groups
        csv2groups = session0.query(CSGroups)
        #logging.debug(type(csv2groups))
        #log.debug(type(csv2groups[0]),csv2groups[0])
        for csgroup in csv2groups:
            log.debug("Dealing with current group: %s", csgroup.group_name)

            # Setup the resources for group and sort out their group and cloud specific yamls
            Base = automap_base()
            engine = create_engine("mysql+pymysql://" + csconfig.config.db_user + ":" + csconfig.config.db_password + "@" +
                     csconfig.config.db_host + ":" + str(csconfig.config.db_port) + "/" + csconfig.config.db_name)
            Base.prepare(engine, reflect=True)
            session = Session(engine)

            Resources = Base.classes.csv2_group_resources
            group_resources = session.query(Resources).filter(Resources.group_name == csgroup.group_name)
            Group_Yaml = Base.classes.csv2_group_yaml
            group_yamls = session.query(Group_Yaml).filter(Group_Yaml.group_name == csgroup.group_name)
            group_yaml_list = []
            for yam in group_yamls:
                group_yaml_list.append([yam.yaml_name, yam.yaml, yam.mime_type])
            cm_group = cloudscheduler.cloudmanager.CloudManager(name=csgroup.group_name, group_resources=group_resources, group_yamls=group_yaml_list)
            cm_group.setup()
            # At this point I should have a valid set of all the group and cloud specific yaml to use
            # it will still need to be combined with the job yaml - done now in the basecloud prepare_userdata function

            # Clean up the idle machines and unregistered VMs.
            # Should return or build up some meta data about what's booting/running
            # To use that info when scheduling idle jobs
            # ie just booted machines should register soon so don't boot more VMs for a small number of jobs.
            idle_machines = check_idle_machines(csgroup.group_name)
            new_vms = check_unregistered_machines(csgroup.group_name)
            current_flavors = count_cloud_flavors(csgroup.group_name)


            # Booting up new VMs to fill in any free space on available clouds related to idle queued jobs
            # Get the idle jobs for the current group
            idle_jobs_for_group = session.query(view_groups_of_idle_jobs).filter(
                view_groups_of_idle_jobs.c.group_name == csgroup.group_name).order_by(
                view_groups_of_idle_jobs.c.job_priority)
            log.debug('---------------got idle jobs--------------')
            for idle_job in idle_jobs_for_group:
                log.debug("Info for current job: Group: %s, Target: %s, User: %s, Flavors: %s",idle_job.group_name, idle_job.target_clouds,
                          idle_job.user, idle_job.flavors)
                # flavors is generated line based on which flavor is the best fit and which clouds have available resources
                # the format is "Group:Cloud:Flavor, Group:Cloud:Flavor" will need to split and use to filter in resources_matching
                job_count = idle_job.idle # number of jobs with identical request
                enough_booting = False
                if idle_job.flavors:
                    flavor_list = [x.strip() for x in idle_job.flavors.split(',')]
                else:
                    continue
                if idle_job.target_clouds:
                    target_cloud_list = [x.strip() for x in idle_job.target_clouds.split(',')]
                else:
                    target_cloud_list = []

                # Get All the possible matching clouds
                clouds_matching = session.query(view_available_resources).filter(
                    view_available_resources.c.flavor.in_(flavor_list),
                    view_available_resources.c.cloud_name.in_(target_cloud_list))
                num_jobs_per_flavor_possible = {}
                for cloud_match in clouds_matching:
                    cpu = cloud_match.flavor_cores / idle_job.request_cpus
                    ram = cloud_match.flavor_ram / idle_job.request_ram
                    disk = cloud_match.flavor_disk / (idle_job.request_disk / 1000000.0) # convert the condor number to Gb
                    jobs_possible = int(min([cpu,ram,disk]))
                    if jobs_possible < 1:
                        jobs_possible = 1
                    num_jobs_per_flavor_possible[cloud_match.flavor] = jobs_possible
                # Now have the amount of jobs that can run on each option
                # Next get all the VMs for this group with their flavor info - actually could be done outside of loop and re-use it
                # Need a structure of the groups VMs accessible by the cloud_name and with a count of the flavor
                # ie vms[cloud-name][flavor] = 6
                runnable_job_count = 0
                for flavor in num_jobs_per_flavor_possible.keys():
                    (group, cloud_name, flavor_name) = flavor.split(':')
                    runnable_job_count += num_jobs_per_flavor_possible[flavor] * current_flavors[cloud_name][flavor_name]
                log.debug("Runnable_job_count before: %s", runnable_job_count)
                runnable_job_count -= idle_job.running # Subtract the already running jobs
                runnable_job_count -= idle_job.idle # subtract the idle jobs in case there's unregistered, unmatched, or poller delay
                log.debug("Runnable_job_count after: %s", runnable_job_count)
                log.debug("Running: %s, Idle: %s", idle_job.running, idle_job.idle)
                if runnable_job_count <= 0: # Need more VMs - try to boot stuff
                    clouds_matching = session.query(view_available_resources).filter(
                        view_available_resources.c.flavor.in_(flavor_list),
                        view_available_resources.c.cloud_name.in_(target_cloud_list))
                    for cloud in clouds_matching:
                        # Need to redo this calculation for each cloud / flavor because it may change
                        num_vms_to_boot = abs(runnable_job_count) / num_jobs_per_flavor_possible[cloud.flavor]
                        # This is the total wanted, but cloud may not be able to handle that request
                        num_vms_to_boot = num_vms_to_boot if num_vms_to_boot <= cloud.flavor_slots else cloud.flavor_slots
                        log.debug("Would like to boot %s VMs on %s", num_vms_to_boot, cloud.cloud_name)
                        try:
                            usertmp = idle_job.user.split('@')[0]
                        except:
                            usertmp = idle_job.user
                        template_dict = {'cs_user': usertmp,
                                         'cs_group_name': csgroup.group_name,
                                         'cs_condor_host': csgroup.condor_central_manager}
                        log.debug(template_dict)
                        try:
                            if cm_group.clouds[cloud_match.cloud_name].enabled:
                                cm_group.clouds[cloud_match.cloud_name]\
                                    .vm_create(group_yaml_list=copy.deepcopy(cm_group.group_yamls),
                                               num=int(num_vms_to_boot),
                                               flavor=cloud.flavor, #cloud_match.flavor_name, # pass 'tri' instead so it keeps the group:cloud:flavor info?
                                               job=idle_job,
                                               template_dict=template_dict)
                            runnable_job_count += num_vms_to_boot * num_jobs_per_flavor_possible[cloud.flavor]
                            log.debug('done booting on cloud %s', cloud_match.cloud_name)
                            if runnable_job_count < 0: # Still need more
                                log.debug("Runnable total still negative after booting VMs, try to boot more on another cloud if possible.")
                                continue
                            else:
                                log.debug("Seem to have booted enough for this job for now.")
                                break

                        except Exception as ex:
                            # lets try to disable or dump this cloud for now
                            log.exception("Disable cloud %s due to exception(later).", cloud_match.cloud_name)
                            #cm_group.clouds[cloud_match.cloud_name].enabled = False
                            # TODO Need logic for re-enable
                else: # We have enough, wait for things to register
                    log.debug("Seem to have enough VMs for the number of idle jobs.")
                    continue
        time.sleep(15) # delay between groups




"""
                # Older algorithm - boots up extra VMs and doesn't account for jobs per vm
                for tri in flavor_list:
                    if enough_booting:
                        break
                    (group, cloud, flavor) = tri.split(':')

                    clouds_matching = session.query(view_available_resources).filter(
                        view_available_resources.c.flavor == tri,
                        view_available_resources.c.cloud_name.in_(target_cloud_list)) # awkward syntax but works.
                    for cloud_match in clouds_matching:
                        log.debug("-------------- %s -------------", cloud_match.cloud_name)
                        if cloud_match.cloud_name not in target_cloud_list:
                            log.debug("Not a target cloud skipping.")
                            continue

                        # how many VMs should I try to boot? lets just go for max we can for now until we add config for limits
                        num_vms_to_boot = job_count if job_count <= cloud_match.flavor_slots else cloud_match.flavor_slots
                        booting = 0
                        for new_vm in new_vms[cloud_match.cloud_name]:
                            # TODO I also need to check the flavor cores against the job request cores when determine the count.
                            if new_vm[2] == cloud_match.flavor_name:
                                booting += 1
                        log.debug("Would like to boot %s, but %s already booting of %s on %s.",
                                  num_vms_to_boot, booting, cloud_match.flavor_name, cloud_match.cloud_name)
                        num_vms_to_boot -= booting
                        if cloud_match.cloud_name in idle_machines.keys():
                            log.debug("Idle slots detected for these jobs: %s", idle_machines[cloud_match.cloud_name])
                            num_vms_to_boot -= idle_machines[cloud_match.cloud_name]
                        if tri in current_machines.keys():
                            num_vms_to_boot -= current_machines[tri]
                            log.debug("Found: %s . Machines with: %s flavor already, deducting from request amount.", current_machines[tri], tri)
                        if num_vms_to_boot < 1:
                            # We don't want to boot anything until the current ones register
                            enough_booting = True
                            break
                        try:
                            usertmp = idle_job.user.split('@')[0]
                        except:
                            usertmp = idle_job.user
                        template_dict = {'cs_user': usertmp,
                                         'cs_group_name': csgroup.group_name,
                                         'cs_condor_host': csgroup.condor_central_manager}
                        log.debug(template_dict)
                        try:
                            if cm_group.clouds[cloud_match.cloud_name].enabled:
                                cm_group.clouds[cloud_match.cloud_name]\
                                    .vm_create(group_yaml_list=copy.deepcopy(cm_group.group_yamls),
                                               num=num_vms_to_boot,
                                               flavor=tri, #cloud_match.flavor_name, # pass 'tri' instead so it keeps the group:cloud:flavor info?
                                               job=idle_job,
                                               template_dict=template_dict)
                            job_count -= num_vms_to_boot
                            log.debug('done booting on cloud %s', cloud_match.cloud_name)
                        except Exception as ex:
                            # lets try to disable or dump this cloud for now
                            log.exception("Disable cloud %s due to exception.", cloud_match.cloud_name)
                            cm_group.clouds[cloud_match.cloud_name].enabled = False
"""
            # Done Trying to schedule idle jobs on free resources.
            # Other tasks the scheduling loop needs to handle for a group?
            # Will need to see about freeing space for higher priority jobs that are not running.

            # Repeat for next group
        #break # will need to take this out and put in an actual stopping condition.



def count_cloud_flavors(group):
    """Query the VMs for group and arrange into a dictionary format
    of dict[cloud_name][flavor] = count of that flavor
    """
    log = logging.getLogger(__name__)
    vms_dict = defaultdict(int)
    base = automap_base()
    engine = create_engine("mysql+pymysql://" + csconfig.config.db_user + ":" + csconfig.config.db_password + "@" +
                           csconfig.config.db_host + ":" + str(csconfig.config.db_port) + "/" + csconfig.config.db_name)
    base.prepare(engine, reflect=True)
    session = Session(engine)
    group_vms = session.query(view_vms).filter(view_vms.c.group_name == group)
    vm_cloud_flavors = defaultdict(lambda: defaultdict(int))
    for vm in group_vms:
        vm_cloud_flavors[vm.cloud_name][vm.flavor_name] += 1
    return vm_cloud_flavors


def check_registered_condor_machines(group):
    """Query the condor machines for everything belonging to the group to get idea
    of how many available resources there are for idle jobs - including brand new machines
    that are in the benchmarking state etc. and use that info to determine if more VMs are needed."""
    log = logging.getLogger(__name__)
    machines_dict = defaultdict(int)
    base = automap_base()
    engine = create_engine("mysql+pymysql://" + csconfig.config.db_user + ":" + csconfig.config.db_password + "@" +
                           csconfig.config.db_host + ":" + str(csconfig.config.db_port) + "/" + csconfig.config.db_name)
    base.prepare(engine, reflect=True)
    session = Session(engine)
    machines = base.classes.condor_machines
    registered_machines = session.query(machines).filter(machines.group_name == group)
    for machine in registered_machines:
        log.debug(machine)
        machines_dict[machine.flavor] += 1
    log.debug(machines_dict)
    return machines_dict

def check_idle_machines(group):
    """
    Query the condor machines and ones that are idle with no jobs requiring them
    or that have been idle an extended period of time and set the Retire flag in db for them.
    :return:
    """
    log = logging.getLogger(__name__)
    log.debug("Check Idle Machines: %s", group)
    Base = automap_base()
    engine = create_engine("mysql+pymysql://" + csconfig.config.db_user + ":" + csconfig.config.db_password + "@" +
                           csconfig.config.db_host + ":" + str(csconfig.config.db_port) + "/" + csconfig.config.db_name)
    Base.prepare(engine, reflect=True)
    session = Session(engine)
    machines = Base.classes.condor_machines
    unclaimed_idle_machines = session.query(machines).filter(machines.state == "Unclaimed",
                                                             machines.activity == "Idle",
                                                             machines.group_name == group)
    machines_dict = defaultdict(int)
    for machine in unclaimed_idle_machines:
        if machine.my_current_time - machine.entered_current_state > 1800 and not machine.condor_off : # TODO make configurable setting for now ~30minutes
            log.debug("Setting condor off flag for machine: %s", machine.name)
            machine.condor_off = 1
        elif machine.condor_off and machine.condor_off == 1:
            pass # We've set the flag - waiting on poller to deal with it.
        elif machine.condor_off and machine.condor_off == 2:
            pass
            # TODO Add additional handling if machine is still here after a long period of time
            # like reset the flag back to 1 so the poller can try the retire again.
            # Need to check if the poller updates anything else when this happens so have something to time against.
        else:
            log.debug("Ignoring classad for: %s, flag: %s Inactive for only %s", machine.name, str(machine.condor_off),
                      str(machine.my_current_time - machine.entered_current_state))
            name_parts = machine.machine.split('--')
            if len(name_parts) >= 2:
                machines_dict[name_parts[1]] += 1 # key on the cloud_name part of hostname

    session.commit()
    # Return dict of cloud names with number of idle machine classads
    return machines_dict


def check_unregistered_machines(group):
    """Query the condor machines and cloud VMs to sort out which ones have failed to register correctly.
    and take steps to shut those down. There could be problems with shutting down machines cs isn't controlling
    so need to account for the hostnames matching the CS hostname pattern, belonging to correct group etc."""
    log = logging.getLogger(__name__)
    log.debug("Check Unregistered Machines: %s", group)
    Base = automap_base()
    engine = create_engine("mysql+pymysql://" + csconfig.config.db_user + ":" + csconfig.config.db_password + "@" +
                           csconfig.config.db_host + ":" + str(csconfig.config.db_port) + "/" + csconfig.config.db_name)
    Base.prepare(engine, reflect=True)
    session = Session(engine)
    machines = Base.classes.condor_machines
    vms = Base.classes.csv2_vms
    clouds = Base.classes.csv2_group_resources
    group_clouds = session.query(clouds).filter(clouds.group_name == group)
    unregistered_vms = session.query(view_vms).filter(view_vms.c.group_name == group).filter(~exists().where(machines.machine.contains(view_vms.c.hostname)))

    cloud_names = []
    for cloud in group_clouds:
        cloud_names.append(cloud.cloud_name)
    non_cs_vm = []
    new_vms = defaultdict(list)
    to_terminate = []
    log.debug("VMs that are unregistered with condor collector:")
    for vm in unregistered_vms:
        try:
            hostname_split = vm.hostname.split('--')
            if len(hostname_split) == 1:
                continue
            cname = hostname_split[1]
            if cname in cloud_names: # name prefix matches a valid cloud name
                if int(time.time()) - vm.status_changed_time > 2400 and not vm.terminate: # not registered after some period of time - TODO make configurable.
                    to_terminate.append(vm.vmid)
                    log.debug("Set terminate flag on vm:%s", vm.hostname)
                else:
                    log.debug("%s only been unregistered for: %s time.", vm.hostname, str(int(time.time()) - vm.status_changed_time))
                    new_vms[vm.cloud_name].append((vm.cloud_name,vm.hostname,vm.flavor_name))
            else:
                non_cs_vm.append(vm.hostname)
        except Exception as ex:
            log.exception("Problem going through unregistered VMs: %s", ex)
    if to_terminate:
        log.debug((len(to_terminate)))
        log.debug(to_terminate)
        update_result = session.query(vms).filter(vms.vmid.in_(to_terminate))
        for vm in update_result:
            vm.terminate = 1
    log.debug("Done checking unregistered VMs.")
    session.commit()
    return new_vms


# should re-adapt this to deal with adding group from DB and splitting code into functions instead of all inline
def add_user_cloud(user, user_resource_file=None):
    """

    :param user:
    :param user_resource_file:
    """
    if not user_resource_file:
        homedir = os.path.expanduser(''.join['~', user])
        user_resource_file = ''.join([homedir, '/.cloudscheduler/cloudresources.yaml'])
    cmuser = cloudscheduler.cloudmanager.CloudManager(name=user, resource_file=user_resource_file)
    cmuser.setup()
    CLOUDRESOURCES[cmuser.name] = cmuser


main()
